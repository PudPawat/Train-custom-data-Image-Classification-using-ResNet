{"cells":[{"metadata":{"_cell_guid":"c3b96391-2148-43c8-9eb1-6f53c9c8a704","_uuid":"2c2c966557b24c189b2e0688ea2c27bacae5be5b"},"cell_type":"markdown","source":"# Using a Pretrained VGG16 to classify retinal damage from OCT Scans\n\n## Motivation and Context\n\n[Transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) turns out to be useful when dealing with relatively small datasets; for examples medical images, which are harder to obtain in large numbers than other datasets. \nInstead of training a deep neural network from scratch, which would require a significant amount of data, power and time, it's often convenient to use a pretrained model and just finetune its performance to simplify and speed up the process.\n\n*If you are new to convolutional neural networks (CNNs) be sure to check out the [wikipedia page on CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network) for a more detailed description*\n\nIn short, convolutional networks are used on (but not only)  images instead of fully-connected feedforward networks because they would require a very high number of neurons - i.e. at least one per pixel in the input layer - and that would make them inconvenient.\n\nCNNs solve this problem by using a different architecture, summarized in Figure 1\n\n![**Figure 1:** CNN architecture](https://it.mathworks.com/content/mathworks/it/it/discovery/convolutional-neural-network/_jcr_content/mainParsys/image_copy.adapt.full.high.jpg/1523891801700.jpg)\n\n**Figure 1:** CNN architecture (from https://it.mathworks.com/discovery/convolutional-neural-network.html) \n\nWe can identify two main blocks inside of a typical CNN: \n - Feature extraction\n - Classification\n \nThe feature extraction is made of a series of convolutional and pooling layers which extract features from the image, increasing in complexity in each layer (i.e. from simpler features in the first layers as points, to more complex ones in the last layers like edges and shapes; see more at [Link #1](http://cs231n.github.io/understanding-cnn) and [Link #2](https://github.com/utkuozbulak/pytorch-cnn-visualizations)).\n\nThese features are then fed to a fully connected network (classifier), which learns to classify them.\n\n*Read more about CNNs at http://cs231n.github.io/convolutional-networks/*\n\nSo, what we'll do here is using a model (VGG-16)  which is already capable of extracting features from an image and train its fully connected network in order to classify different types of retinal damage instead of objects.\n\nThe model we'll use is a [VGG-16](https://www.quora.com/What-is-the-VGG-neural-network)- convolutional network, trained on [ImageNet](http://www.image-net.org/) dataset. \n\nThis work will use [PyTorch](https://pytorch.org/) as deep learning framework and [CUDA](https://developer.nvidia.com/cuda-zone) for GPU acceleration. "},{"metadata":{"_cell_guid":"a2276b70-8eae-4709-9af1-e08df6bab2c1","_uuid":"0fca07bf66c325a944b4866601538e7c4e976e50"},"cell_type":"markdown","source":"## About Retinal OCT\n\n*Quoted from https://www.kaggle.com/paultimothymooney/kermany2018 *\n\nRetinal optical coherence tomography (OCT) is an imaging technique used to capture high-resolution cross sections of the retinas of living patients. Approximately 30 million OCT scans are performed each year, and the analysis and interpretation of these images takes up a significant amount of time (Swanson and Fujimoto, 2017).\n\n![Figure 2](https://i.imgur.com/fSTeZMd.png)\n\nFigure 2. Representative Optical Coherence Tomography Images and the Workflow Diagram [Kermany et. al. 2018] http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n\n(A) (Far left) choroidal neovascularization (CNV) with neovascular membrane (white arrowheads) and associated subretinal fluid (arrows). (Middle left) Diabetic macular edema (DME) with retinal-thickening-associated intraretinal fluid (arrows). (Middle right) Multiple drusen (arrowheads) present in early AMD. (Far right) Normal retina with preserved foveal contour and absence of any retinal fluid/edema."},{"metadata":{"_cell_guid":"26359bdf-00a9-4280-9ec4-6ca7295148e4","_uuid":"108609d6b29e75ec9f0fe15f66b4336f694c74b2"},"cell_type":"markdown","source":"## Implementation"},{"metadata":{"_cell_guid":"b4eb5ad5-03ba-4fc4-b417-451e30a01fd1","_uuid":"be3002e45fc687fc33e7c7ae9d869ac7ee926b1a"},"cell_type":"markdown","source":"*Python Modules*"},{"metadata":{"_cell_guid":"0e05b5f8-fc51-404d-a9d2-5197aa283b73","_uuid":"76fd0ec2a5eb7fbe49b51147eabbd109c61279c0","trusted":false,"collapsed":true},"cell_type":"code","source":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\nplt.ion()  \n\nuse_gpu = torch.cuda.is_available()\nif use_gpu:\n    print(\"Using CUDA\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07763a82-f724-46ec-a323-e1f26101dcf3","_uuid":"c060d3a7eae2502855e84ae5eabd1a8c72fc4ea2","colab_type":"text","id":"69iuBpidwheJ"},"cell_type":"markdown","source":"## Dataset loader\n\nThe dataset is divided in three categories: training, validation and test. \n\nThe first one will be, obviously, used for trainig; the validation set will be used to measure the model performance during training and the test set will be used to evaluate our model performance once the training has finished.\n\n*Note:* These three sets should all contain different images.\n\nLoading this dataset with pytorch is really easy using [ImageFolder](https://pytorch.org/docs/master/torchvision/datasets.html#imagefolder) as the labels are specified by the folders names.  "},{"metadata":{"_cell_guid":"e9d7ef88-fdbd-4b73-b64f-70294976d238","colab_type":"code","id":"xvsy0IR4wheJ","executionInfo":{"elapsed":1074,"user":{"displayName":"Carlo Alberto","userId":"107843268563316278814","photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg"},"timestamp":1525007461873,"user_tz":-120,"status":"ok"},"_uuid":"83371f19c7f1e261bb0f9cc71f7a265c37a4c16a","colab":{"base_uri":"https://localhost:8080/","height":68,"autoexec":{"startup":false,"wait_interval":0}},"outputId":"bb02efaa-518c-4342-d6e5-7275a7d7fdd5","trusted":false,"collapsed":true},"cell_type":"code","source":"data_dir = '../input/kermany2018/oct2017/OCT2017 '\nTRAIN = 'train'\nVAL = 'val'\nTEST = 'test'\n\n# VGG-16 Takes 224x224 images as input, so we resize all of them\ndata_transforms = {\n    TRAIN: transforms.Compose([\n        # Data augmentation is a good practice for the train set\n        # Here, we randomly crop the image to 224x224 and\n        # randomly flip it horizontally. \n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ]),\n    TEST: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ])\n}\n\nimage_datasets = {\n    x: datasets.ImageFolder(\n        os.path.join(data_dir, x), \n        transform=data_transforms[x]\n    )\n    for x in [TRAIN, VAL, TEST]\n}\n\ndataloaders = {\n    x: torch.utils.data.DataLoader(\n        image_datasets[x], batch_size=8,\n        shuffle=True, num_workers=4\n    )\n    for x in [TRAIN, VAL, TEST]\n}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n\nfor x in [TRAIN, VAL, TEST]:\n    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n    \nprint(\"Classes: \")\nclass_names = image_datasets[TRAIN].classes\nprint(image_datasets[TRAIN].classes)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ad1e66d-dcfc-429a-9664-e61b4d2944cc","_uuid":"80417b6d4f2000be40245a090424b2452e1b78e1","colab_type":"text","id":"6f1fb14iwheO"},"cell_type":"markdown","source":"## Utils\n\nSome utility function to visualize the dataset and the model's predictions"},{"metadata":{"_cell_guid":"fd84399d-83f9-4af7-8767-fe1d32856493","colab_type":"code","id":"rphPgOQewheQ","executionInfo":{"elapsed":1443,"user":{"displayName":"Carlo Alberto","userId":"107843268563316278814","photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg"},"timestamp":1525007467056,"user_tz":-120,"status":"ok"},"_uuid":"48e7f7c02cab559638af532e98d371ebd8c89bfa","colab":{"base_uri":"https://localhost:8080/","height":119,"autoexec":{"startup":false,"wait_interval":0}},"outputId":"0bec4c14-9968-4119-bb75-f896ddc21ebd","trusted":false,"collapsed":true},"cell_type":"code","source":"def imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\n\ndef show_databatch(inputs, classes):\n    out = torchvision.utils.make_grid(inputs)\n    imshow(out, title=[class_names[x] for x in classes])\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders[TRAIN]))\nshow_databatch(inputs, classes)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e6e0c3f-19bd-4034-b408-eeffb9746275","colab_type":"code","id":"WXgC3SYkwheT","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"d7785c091b61ab0ddff8556c06bafc5f16037aa4","collapsed":true,"trusted":false},"cell_type":"code","source":"def visualize_model(vgg, num_images=6):\n    was_training = vgg.training\n    \n    # Set model for evaluation\n    vgg.train(False)\n    vgg.eval() \n    \n    images_so_far = 0\n\n    for i, data in enumerate(dataloaders[TEST]):\n        inputs, labels = data\n        size = inputs.size()[0]\n        \n        if use_gpu:\n            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n        else:\n            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n        \n        outputs = vgg(inputs)\n        \n        _, preds = torch.max(outputs.data, 1)\n        predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n        \n        print(\"Ground truth:\")\n        show_databatch(inputs.data.cpu(), labels.data.cpu())\n        print(\"Prediction:\")\n        show_databatch(inputs.data.cpu(), predicted_labels)\n        \n        del inputs, labels, outputs, preds, predicted_labels\n        torch.cuda.empty_cache()\n        \n        images_so_far += size\n        if images_so_far >= num_images:\n            break\n        \n    vgg.train(mode=was_training) # Revert model back to original training state","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d3ad92fa-f55c-4542-ba7a-55f2dad9cf97","_uuid":"d4aa323ffa73c1af0425163096f06ae548f39a2b"},"cell_type":"markdown","source":"This helper function will give us the accuracy of our model on the test set."},{"metadata":{"_cell_guid":"1b74302a-9418-4472-970f-76591d00cecb","_uuid":"6d1b1cd49e177152940d8f667b9fa5e2e1c5e360","collapsed":true,"trusted":false},"cell_type":"code","source":"def eval_model(vgg, criterion):\n    since = time.time()\n    avg_loss = 0\n    avg_acc = 0\n    loss_test = 0\n    acc_test = 0\n    \n    test_batches = len(dataloaders[TEST])\n    print(\"Evaluating model\")\n    print('-' * 10)\n    \n    for i, data in enumerate(dataloaders[TEST]):\n        if i % 100 == 0:\n            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n\n        vgg.train(False)\n        vgg.eval()\n        inputs, labels = data\n\n        if use_gpu:\n            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n        else:\n            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n\n        outputs = vgg(inputs)\n\n        _, preds = torch.max(outputs.data, 1)\n        loss = criterion(outputs, labels)\n\n        loss_test += loss.data[0]\n        acc_test += torch.sum(preds == labels.data)\n\n        del inputs, labels, outputs, preds\n        torch.cuda.empty_cache()\n        \n    avg_loss = loss_test / dataset_sizes[TEST]\n    avg_acc = acc_test / dataset_sizes[TEST]\n    \n    elapsed_time = time.time() - since\n    print()\n    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n    print('-' * 10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cdd5ab4-d82e-4c1f-abc0-4f36bae571ee","_uuid":"6d11a677fe7c58615b400e6aef8173c911989932","colab_type":"text","id":"jF7K6W9BwheW"},"cell_type":"markdown","source":"## Model creation\n\nThe [VGG-16](https://www.quora.com/What-is-the-VGG-neural-network) is able to classify 1000 different labels; we just need 4 instead. \nIn order to do that we are going replace the last fully connected layer of the model with a new one with 4 output features instead of 1000. \n\nIn PyTorch, we can access the VGG-16 classifier with `model.classifier`, which is an 6-layer array. We will replace the last entry.\n\nWe can also disable training for the convolutional layers setting `requre_grad = False`, as we will only train the fully connected classifier."},{"metadata":{"_cell_guid":"4be764f7-24ff-4611-8fc6-31b87e3ed171","colab_type":"code","id":"SjHLMTldwheY","executionInfo":{"elapsed":2623,"user":{"displayName":"Carlo Alberto","userId":"107843268563316278814","photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg"},"timestamp":1525007474565,"user_tz":-120,"status":"ok"},"_uuid":"480b7181f00142865d3e971c799dd42bc9d1f7e5","colab":{"base_uri":"https://localhost:8080/","height":1003,"autoexec":{"startup":false,"wait_interval":0}},"outputId":"4c40caae-9d25-47e5-fe17-a4f3f944487e","trusted":false,"collapsed":true},"cell_type":"code","source":"# Load the pretrained model from pytorch\nvgg16 = models.vgg16_bn()\nvgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\nprint(vgg16.classifier[6].out_features) # 1000 \n\n\n# Freeze training for all layers\nfor param in vgg16.features.parameters():\n    param.require_grad = False\n\n# Newly created modules have require_grad=True by default\nnum_features = vgg16.classifier[6].in_features\nfeatures = list(vgg16.classifier.children())[:-1] # Remove last layer\nfeatures.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\nvgg16.classifier = nn.Sequential(*features) # Replace the model classifier\nprint(vgg16)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"731d8833-c6d1-4cba-996c-882ccf547f95","_uuid":"cc4ce91edbd602335e92505a269de62e077385ec"},"cell_type":"markdown","source":"The output above is the summary of our model. Notice how the last layer has 4 output features as we specified. "},{"metadata":{"_cell_guid":"f45bda67-096e-4de9-a801-98742c34207c","colab_type":"code","id":"Xuaq38UOwhec","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"bd59ea5c500e55b3ae4bd9cdfeae01bd50818668","collapsed":true,"trusted":false},"cell_type":"code","source":"# If you want to train the model for more than 2 epochs, set this to True after the first run\nresume_training = False\n\nif resume_training:\n    print(\"Loading pretrained model..\")\n    vgg16.load_state_dict(torch.load('../input/vgg16-transfer-learning-pytorch/VGG16_v2-OCT_Retina.pt'))\n    print(\"Loaded!\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"25456a84-cf7a-4605-8897-ebe5eda2e7ff","_uuid":"b4454911af7ec6b1045003a25682bcffe3946e0e"},"cell_type":"markdown","source":"We will also define our loss function (cross entropy) and the optimizer. \n\nThe learning rate will start at 0.001 and a StepLR object will decrese it by a factor of 0.1 every 7 epochs (not really needed here as we're training for 2 epochs only)\n\nFor further details refer to https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n"},{"metadata":{"_cell_guid":"df23921b-f26e-496a-9a71-cdf2a9daa34e","colab_type":"code","id":"HTJWo25nwhef","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"45d872d00fcfe93de8938b8741b4526af971c62b","collapsed":true,"trusted":false},"cell_type":"code","source":"if use_gpu:\n    vgg16.cuda() #.cuda() will move everything to the GPU side\n    \ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"996330eb-394c-4916-a924-8d77bc2ffb5b","_uuid":"5ced4fda7b1a2f175f8795eb5b4d8a1a66c76497","colab_type":"text","id":"MuVPA5gkwhen"},"cell_type":"markdown","source":"## Model evaluation and visualization (before training)\n\nLet's see how our model performs before any training"},{"metadata":{"_cell_guid":"f7aec745-2087-4f60-987d-a34a995fd6a9","colab_type":"code","id":"jSa-X3XVwheo","executionInfo":{"elapsed":1986,"user":{"displayName":"Carlo Alberto","userId":"107843268563316278814","photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg"},"timestamp":1525007488210,"user_tz":-120,"status":"ok"},"_uuid":"7732406aae91a82348fcb830a5ff5785d22526fb","colab":{"base_uri":"https://localhost:8080/","height":306,"autoexec":{"startup":false,"wait_interval":0}},"outputId":"d6f338d4-5379-4fe8-aaec-cc8141b7cbb5","collapsed":true,"trusted":false},"cell_type":"code","source":"print(\"Test before training\")\neval_model(vgg16, criterion)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dabed264-9a90-4888-ad3c-65924ad9c80d","_uuid":"eff4aa2d97fd5443195ae2a2cd43c5f73e6775a3","trusted":false,"collapsed":true},"cell_type":"code","source":"visualize_model(vgg16) #test before training","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1d7a980-d27f-4637-a04e-abfbe67464fb","_uuid":"464dc207138f6042bf8b9b1e33d5c792c00b8dac"},"cell_type":"markdown","source":"Not really great results. Let's see if it can do better after training "},{"metadata":{"_cell_guid":"46ffdcaf-049a-4dbe-ba6d-b25ccc4ff448","_uuid":"d157eb6d3f1f94af2a369bff6e3e5277482ea6c2","colab_type":"text","id":"xDUpsi7cwhes"},"cell_type":"markdown","source":"## Training\n\nWhat follows is [pretty standard pytorch code for training](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html). \n\nFor every epoch we iterate over all the training batches, compute the loss , and adjust the network weights with `loss.backward()` and `optimizer.step()`. \nThen we evaluate the performance over the validaton set. At the end of every epoch we print the network progress (loss and accuracy). \nThe accuracy will tell us how many predictions were correct.\n\nAs we said before, transfer learning can work on smaller dataset too, so for every epoch we only iterate over half the trainig dataset (worth noting that it won't exactly be half of it over the entire training, as the data is shuffled, but it will almost certainly be a subset) "},{"metadata":{"_cell_guid":"8f6936eb-ae6f-44ee-90a9-c7f817ba6eda","colab_type":"code","id":"lHkiBU5fwhet","colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"abe5dc35b31e7971e7d63637866132f89e7d011d","collapsed":true,"trusted":false},"cell_type":"code","source":"def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    best_model_wts = copy.deepcopy(vgg.state_dict())\n    best_acc = 0.0\n    \n    avg_loss = 0\n    avg_acc = 0\n    avg_loss_val = 0\n    avg_acc_val = 0\n    \n    train_batches = len(dataloaders[TRAIN])\n    val_batches = len(dataloaders[VAL])\n    \n    for epoch in range(num_epochs):\n        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n        print('-' * 10)\n        \n        loss_train = 0\n        loss_val = 0\n        acc_train = 0\n        acc_val = 0\n        \n        vgg.train(True)\n        \n        for i, data in enumerate(dataloaders[TRAIN]):\n            if i % 100 == 0:\n                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n                \n            # Use half training dataset\n            if i >= train_batches / 2:\n                break\n                \n            inputs, labels = data\n            \n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n            else:\n                inputs, labels = Variable(inputs), Variable(labels)\n            \n            optimizer.zero_grad()\n            \n            outputs = vgg(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            loss_train += loss.data[0]\n            acc_train += torch.sum(preds == labels.data)\n            \n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        print()\n        # * 2 as we only used half of the dataset\n        avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n        avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n        \n        vgg.train(False)\n        vgg.eval()\n            \n        for i, data in enumerate(dataloaders[VAL]):\n            if i % 100 == 0:\n                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n                \n            inputs, labels = data\n            \n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n            else:\n                inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n            \n            optimizer.zero_grad()\n            \n            outputs = vgg(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            loss_val += loss.data[0]\n            acc_val += torch.sum(preds == labels.data)\n            \n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        avg_loss_val = loss_val / dataset_sizes[VAL]\n        avg_acc_val = acc_val / dataset_sizes[VAL]\n        \n        print()\n        print(\"Epoch {} result: \".format(epoch))\n        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n        print('-' * 10)\n        print()\n        \n        if avg_acc_val > best_acc:\n            best_acc = avg_acc_val\n            best_model_wts = copy.deepcopy(vgg.state_dict())\n        \n    elapsed_time = time.time() - since\n    print()\n    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n    print(\"Best acc: {:.4f}\".format(best_acc))\n    \n    vgg.load_state_dict(best_model_wts)\n    return vgg","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53eeb478-e106-49be-9682-0173e76640f8","colab_type":"code","id":"r1-I_IUUwhew","executionInfo":{"elapsed":304,"user":{"displayName":"Carlo Alberto","userId":"107843268563316278814","photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg"},"timestamp":1525023518627,"user_tz":-120,"status":"ok"},"_uuid":"a0ddf54b1c45b7c61724cbe1e74ca0628f8b1d8e","colab":{"base_uri":"https://localhost:8080/","height":492,"autoexec":{"startup":false,"wait_interval":0}},"outputId":"b764e48a-00fd-4eb5-f592-f4386a875ed1","collapsed":true,"trusted":false},"cell_type":"code","source":"vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=2)\ntorch.save(vgg16.state_dict(), 'VGG16_v2-OCT_Retina_half_dataset.pt')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f9276b6e-2984-469b-820e-9eb1f7eeced5","_uuid":"7898a0245f8b5c5961945c4a591dfea2ea72ea94"},"cell_type":"markdown","source":"## Model evaluation and visualization (after training)\n\nLet's evaluate our model again after 2 epochs of training"},{"metadata":{"_cell_guid":"f8d905fe-9f7f-484b-b926-9931ca887e34","_uuid":"2803b24b7002a785833d300413e3bd8891f398f9","collapsed":true,"trusted":false},"cell_type":"code","source":"eval_model(vgg16, criterion)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12b4537d-04c1-440f-913b-53ff739b9d63","_uuid":"4daf44a1edec9bc16ef0319ab99f1859373001e7"},"cell_type":"markdown","source":"Now that's a pretty good result!"},{"metadata":{"_cell_guid":"c3f7f06c-97fe-4f8e-8b47-f512d4989ecb","colab_type":"code","id":"hGLvyjZ2whe0","executionInfo":{"elapsed":4437,"user":{"displayName":"Carlo Alberto","userId":"107843268563316278814","photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg"},"timestamp":1525025142751,"user_tz":-120,"status":"ok"},"_uuid":"4f2ed1c58098075949f93eadc7ba3b5786c7b307","colab":{"base_uri":"https://localhost:8080/","height":1122,"autoexec":{"startup":false,"wait_interval":0}},"outputId":"497f6621-ee2c-43a5-9b54-917bbbe9b22c","collapsed":true,"trusted":false},"cell_type":"code","source":"visualize_model(vgg16, num_images=32)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c36cfd33-f857-419d-8508-2a4441c68614","_uuid":"39d5b7019cd132b26b95d250d5983f6ada032c92"},"cell_type":"markdown","source":"## Things to try\n- Reduce the training dataset even further, and increase the test set\n- Implement other evaluation metrics like in https://www.kaggle.com/paultimothymooney/detect-retina-damage-from-oct-images/notebook"},{"metadata":{"_cell_guid":"7885babe-9858-4199-b6cc-df13f507c91b","_uuid":"c83e276974d07d618bea5a0e1f603b9fd9b21193"},"cell_type":"markdown","source":"Hope you found this useful! "}],"metadata":{"colab":{"name":"VGG16_v2-OCT2017_Retina.ipynb","version":"0.3.2","toc_visible":true,"views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU","language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}